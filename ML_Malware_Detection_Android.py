import os
import apk
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn
# Import ML algorithms
from sklearn.model_selection import train_test_split  # import thing that will split the array into train/test subsets
from sklearn.metrics import confusion_matrix  # Allows us to measure performance: gives true/false positives/negatives
from sklearn.preprocessing import StandardScaler  # Scale the features to be b/w 0 and 1 to standardize them
from sklearn.neural_network import MLPClassifier  # A multi-layer perceptron neural network
from sklearn.ensemble import RandomForestClassifier  # Random forests, best classical ML approach to take

# Make a class called apkFile, to store apk files

class apkFile:
    # Constructor for apk class
    def __init__(self, filename):
        self.apkfile = apk.APK(filename)  # make an object, called apkfile, of the APK class from the apk library
        self.filesize = self.apkfile.file_size
        self.cert = self.apkfile.get_certificate(filename)  # google certification
        self.permissions = self.apkfile.get_permissions()  # file permissions
        #self.providers = self.apkfile.get_providers()
        #self.activities = self.apkfile.get_activities()
        #self.services = self.apkfile.get_services()
        #self.receivers = self.apkfile.get_receivers()
        # self.cert_md5 : get md5 hash, see if its helpful ??
    # function that constructs a dictionary entry for an object (individual apk file)

    def construct(self):
        sample = {}
        for attr, k in self.__dict__.iteritems():  # GO OVER THIS CODE W CLINTON
            if (attr != "apk"):
                sample[attr] = k
        return sample

"""apk2vec
 loops through all samples in a folder and extracts them, turns this collection of samples
 into a dataset
"""


def apk2vec(directory):
    dataset = {}
    for subdir, dirs, files in os.walk(directory):
        for f in files:
            file_path = os.path.join(subdir, f)
            try:
                apk = apkFile(file_path) #POSSIBLE ERROR??
                dataset[str(f)] = apk.construct()
            except Exception as e:
                print(e)
    return dataset

"""vec2csv
 Stores all samples in 1 csv file

"""


def vec2csv(dataset, filename):
    df = pd.DataFrame(dataset)
    df = df.transpose()  # transpose to have the features as columns and samples as rows
    df.to_csv(filename, sep=',', encoding='utf-8')  # ASK CLINTON

# The code that calls the functions and makes the two files
directoryPath_Malicious = 'C:\\Polina\\apkCLEAN'  # the directory to index, with mal files
directoryPath_Clean = 'C:\\Polina\\apkCLEAN'  # the clean directory to index

dataset_Malicious = apk2vec(directoryPath_Malicious)
dataset_Clean = apk2vec(directoryPath_Clean)

vec2csv(dataset_Malicious, 'dataset_Malicious.csv')
vec2csv(dataset_Clean, 'dataset_Clean.csv')

malicious = pd.read_csv("dataset_Malicious.csv")
clean = pd.read_csv("dataset_Clean.csv")

# Visualize data using stats - TO BE ADDED
print("Clean Files Statistics")
clean.describe()
print("Malicious Files Statistics")
malicious.describe()

# Visualize data using matplotlib - TO BE ADDED
malicious['clean'] = 0
clean['clean'] = 1
''' 
fig,ax = plt.subplots()
x = malicious['cert']
y = malicious['clean']
ax.scatter(x,y,color='r',label='Malicious')
x1 = clean['IATRVA']
y1 = clean['clean']
ax.scatter(x1,y1,color='b',label='Cleanfiles')
ax.legend(loc="right")

fig,ax = plt.subplots()
x = malicious['DebugRVA']
y = malicious['clean']
ax.scatter(x,y,color='r',label='Malicious')
x1 = clean['DebugRVA']
y1 = clean['clean']
ax.scatter(x1,y1,color='b',label='Cleanfiles')
ax.legend(loc="right")

fig,ax = plt.subplots()
x = malicious['ExportSize']
y = malicious['clean']
ax.scatter(x,y,color='r',label='Malicious')
x1 = clean['ExportSize']
y1 = clean['clean']
ax.scatter(x1,y1,color='b',label='Cleanfiles')
ax.legend(loc="right")
'''
# Merge dataset
dataset_Merged = [malicious, clean]
dataset = pd.concat(dataset_Merged)
#a = type(dataset_Merged);b = type(dataset);
vec2csv(dataset.transpose(), 'dataset_Merged.csv')

# Dataset prep, algorithm implementation

state = np.random.randint(100)
y = dataset['clean']
X = dataset.drop('clean', axis=1)
X = np.asarray(X)
y = np.asarray(y)

#X = X[:, 2:]  # get rid of the zeroth column of X (the filename), and the last column (the file path) -P
#print(X[1, 11])
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0)

# Random Forests
classifier1 = RandomForestClassifier()
classifier1.fit(X_train, y_train)
y_pred = classifier1.predict(X_test)

tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()
print("TN = ", tn)
print("TP = ", tp)
print("FP = ", fp)
print("FN = ", fn)

# MLP
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)
scaler = StandardScaler()
scaler.fit(X_train)
X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)

mlp = MLPClassifier(hidden_layer_sizes=(12, 12, 12, 12, 12, 12))
mlp.fit(X_train, y_train)
predictions = mlp.predict(X_test)

tn, fp, fn, tp = confusion_matrix(y_test, predictions).ravel()
print("TN = ", tn)
print("TP = ", tp)
print("FP = ", fp)
print("FN = ", fn)
